# Awesome-MLLM-Datasets
ðŸš€ðŸš€ðŸš€This project aims to collect and collate various datasets for multimodal large model training, including but not limited to pre-training data, instruction fine-tuning data, and In-Context learning data.

ðŸ’¡ðŸ’¡ðŸ’¡The goal of the project is to provide researchers with a comprehensive repository of resources to support their ability to more easily access high-quality datasets when developing and optimizing multimodal AI systems.

**Table of Contents**
- [Datasets of Pre-Training](#datasets-of-pre-training)
- [Datasets of Multimodal Instruction Tuning](#datasets-of-multimodal-instruction-tuning)
- [Datasets of In-Context Learning](#datasets-of-in-context-learning)
- [Datasets of Multimodal Chain-of-Thought](#datasets-of-multimodal-chain-of-thought)
- [Datasets of Multimodal RLHF](#datasets-of-multimodal-rlhf)
- [Benchmarks for Evaluation](#benchmarks-for-evaluation)

## Datasets of Pre-Training

| Name                           |             #.X             |  #.T  | #.X-T  |                                                                                                                      Paper                                                                                                                       | Link | Type |                                                      Notes                                                      |
|:-------------------------------|:---------------------------:|:-----:|:------:|:------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------:|:----:|------|:---------------------------------------------------------------------------------------------------------------:|
| **WebLI**                      |         10B(Images)         |  12B  |  12B   |                                                                           [PaLI: A Jointly-Scaled Multilingual Language-Image Model](https://arxiv.org/pdf/2209.06794)                                                                           |      |      |                                                                                                                 |
| **LAION-5B**                   |        5.9B(Images)         | 5.9B  |  5.9B  |                                                            [LAION-5B: An open large-scale dataset for training next generation image-text models](https://arxiv.org/pdf/2210.08402v1)                                                            |      |      |                                                                                                                 |
| **LAION-en**                   |        2.3B(Images)         | 2.3B  |  2.3B  |                                                            [LAION-5B: An open large-scale dataset for training next generation image-text models](https://arxiv.org/pdf/2210.08402v1)                                                            |      |      |                                                                                                                 |
| **ALIGN**                      |        1.8B(Images)         | 1.8B  |  1.8B  |                                                         [Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision](https://arxiv.org/pdf/2102.05918v2)                                                          |      |      |                                                                                                                 |
| **DataComp**                   |        1.4B(Images)         | 1.4B  |  1.4B  |                                                                  [DATACOMP: In search of the next generation of multimodal datasets](https://openreview.net/pdf?id=dVaWCDMBof)                                                                   |      |      |                                                                                                                 |
| **COYO**                       |        747M(Images)         | 747M  |  747M  |                                                                           [COYO-700M: Large-scale Image-Text Pair Dataset](https://github.com/kakaobrain/coyo-dataset)                                                                           |      |      |                                                                                                                 |
| **LAION-COCO**                 |        600M(Images)         | 600M  |  600M  |                                                                             [LAION COCO: 600M SYNTHETIC CAPTIONS FROM LAION2B-EN](https://laion.ai/blog/laion-coco/)                                                                             |      |      |                                                                                                                 |
| **LAION-400M**                 |        400M(Images)         | 400M  |  400M  |                                                                   [LAION-400M: Open Dataset of CLIP-Filtered 400 Million Image-Text Pairs](https://arxiv.org/pdf/2111.02114v1)                                                                   |      |      |                                                                                                                 |
| **Episodic WebLI**             |        400M(Images)         | 400M  |  400M  |                                                                        [PaLI-X: On Scaling up a Multilingual Vision and Language Model](https://arxiv.org/pdf/2305.18565)                                                                        |      |      |                                                                                                                 |
| **CLIP**                       |        400M(Images)         | 400M  |  400M  |                                                                   [Learning Transferable Visual Models From Natural Language Supervision](https://arxiv.org/pdf/2106.02524v1)                                                                    |      |      |                                                                                                                 |
| **LTIP**                       |        312M(Images)         | 312M  |  312M  |                                                                          [Flamingo: a Visual Language Model for Few-Shot Learning](https://arxiv.org/pdf/2204.14198v2)                                                                           |      |      |                                                                                                                 |
| **FILIP**                      |        300M(Images)         | 300M  |  300M  |                                                                        [FILIP: Fine-grained Interactive Language-Image Pre-Training](https://arxiv.org/pdf/2111.07783v1)                                                                         |      |      |                                                                                                                 | 
| **LAION-zh**                   |        142M(Images)         | 142M  |  142M  |                                                            [LAION-5B: An open large-scale dataset for training next generation image-text models](https://arxiv.org/pdf/2210.08402v1)                                                            |      |      |                                                                                                                 |
| **Obelics(Interleaved)**       |        353M(Images)         | 115M  |  141M  |                                                           [OBELICS: An Open Web-Scale Filtered Dataset of Interleaved Image-Text Documents](https://openreview.net/pdf?id=SKN2hflBIZ)                                                            |      |      |                                                                                                                 |
| **MMC4(Interleaved)**          |        571M(Images)         |  43B  | 101.2M |                                                                [Multimodal C4: An Open, Billion-scale Corpus of Images Interleaved With Text](https://arxiv.org/pdf/2304.06939v1)                                                                |      |      |                                                                                                                 |
| **Wukong**                     |        101M(Images)         | 101M  |  101M  |                                                      [WuKong:100 Million Large-scale Chinese Cross-modal Pre-training Dataset and A Foundation Framework](https://arxiv.org/pdf/2202.06767)                                                      |      |      |                                                                                                                 |
| **M3W(MultiModal MassiveWeb)** |        185M(Images)         | 182GB | 43.3M  |                       [Flamingo: a Visual Language Model for Few-Shot Learning](https://storage.googleapis.com/deepmind-media/DeepMind.com/Blog/tackling-multiple-tasks-with-a-single-visual-language-model/flamingo.pdf)                        |      |      |                                                                                                                 |
| **WIT**                        |        11.5M(Images)        | 37.6M | 37.6M  |                                                            [WIT: Wikipedia-based Image Text Dataset for Multimodal Multilingual Machine Learning](https://arxiv.org/pdf/2103.01913v2)                                                            |      |      |                                                                                                                 |
| **GQA**                        |        113K(Images)         |  22M  |  22M   | [GQA: A New Dataset for Real-World Visual Reasoning and Compositional Question Answering](https://openaccess.thecvf.com/content_CVPR_2019/papers/Hudson_GQA_A_New_Dataset_for_Real-World_Visual_Reasoning_and_Compositional_CVPR_2019_paper.pdf) |      |      |                                                                                                                 |
| **CC12M**                      |        12.4M(Images)        | 12.4M | 12.4M  |                                                      [Conceptual 12M: Pushing Web-Scale Image-Text Pre-Training To Recognize Long-Tail Visual Concepts](https://arxiv.org/pdf/2102.08981v2)                                                      |      |      |                                                                                                                 |
| **Red Caps**                   |         12M(Images)         |  12M  |  12M   |                                                                 [RedCaps: Web-curated image-text data created by the people, for the people](https://arxiv.org/pdf/2111.11431v1)                                                                 |      |      |                                                                                                                 |
| **Visual Genome**              |        108k(Images)         | 4.5M  |  4.5M  |                                                          [Visual Genome: Connecting Language and Vision Using Crowdsourced Dense Image Annotations](https://arxiv.org/pdf/1602.07332v1)                                                          |      |      |                                                                                                                 |
| **DVQA**                       |        300K(Images)         | 3.5M  |  3.5M  |                                    [DVQA: Understanding Data Visualizations via Question Answering](https://openaccess.thecvf.com/content_cvpr_2018/papers/Kafle_DVQA_Understanding_Data_CVPR_2018_paper.pdf)                                    |      |      |                                                                                                                 |
| **CC3M**                       |        3.3M(Images)         | 3.3M  |  3.3M  |                                                    [Conceptual Captions: A Cleaned, Hypernymed, Image Alt-text Dataset For Automatic Image Captioning](https://aclanthology.org/P18-1238.pdf)                                                    |      |      |                                                                                                                 |
| **MS-COCO**                    |        328k(Images)         | 2.5M  |  2.5M  |                                                                                  [Microsoft COCO: Common Objects in Context](https://arxiv.org/pdf/1405.0312v3)                                                                                  |      |      |                                                                                                                 |
| **AI Challenger Captions**     |        300K(Images)         | 1.5M  |  1.5M  |                                                                [AI Challenger : A Large-scale Dataset for Going Deeper in Image Understanding](https://arxiv.org/pdf/1711.06475)                                                                 |      |      |                                                                                                                 |
| **VQA v2**                     |        265K(Images)         | 1.4M  |  1.4M  |                                                      [Making the V in VQA Matter: Elevating the Role of Image Understanding in Visual Question Answering](https://arxiv.org/pdf/1612.00837)                                                      |      |      |                                                                                                                 |
| **SBU(Image Caption)**         |         1M(Images)          |  1M   |   1M   |                                    [Im2Text: Describing Images Using 1 Million Captioned Photographs](https://proceedings.neurips.cc/paper_files/paper/2011/file/5dd9db5e033da9c6fb5ba83c7a7ebea9-Paper.pdf)                                     |      |      |                                                                                                                 |
| **OCR-VQA**                    |        207K(Images)         |  1M   |   1M   |                                                             [OCR-VQA: Visual Question Answering by Reading Text in Images](https://anandmishra22.github.io/files/mishra-OCR-VQA.pdf)                                                             |      |      |                                                                                                                 |
| **COCO Caption**               |        164K(Images)         |  1M   |   1M   |                                                                        [Microsoft COCO Captions: Data Collection and Evaluation Server](https://arxiv.org/pdf/1504.00325)                                                                        |      |      |                                                                                                                 |
| **CC595k**                     |        595K(Images)         | 595K  |  595K  |                                                                                      [Visual Instruction Tuning](https://openreview.net/pdf?id=w0H2xGHlkw)                                                                                       |      |      |                                                                                                                 |
| **Visual-7W**                  |        47.3K(Images)        | 328K  |  328K  |                                                                              [Visual7W: Grounded Question Answering in Images](https://arxiv.org/pdf/1511.03416v4)                                                                               |      |      |                                                                                                                 |
| **Flickr30k**                  |         31K(Images)         | 158K  |  158K  |                                          [From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions](https://aclanthology.org/Q14-1006.pdf)                                           |      |      |                                                                                                                 |
| **Text Captions**              |         28K(Images)         | 145K  |  145K  |                                                  [TextCaps: a Dataset for Image Captioning with Reading Comprehension](https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123470732.pdf)                                                   |      |      |                                                                                                                 |
| **RefCOCO**                    |         20K(Images)         | 142K  |  142K  |                                                                   [ReferItGame: Referring to Objects in Photographs of Natural Scenes](https://aclanthology.org/D14-1086.pdf)                                                                    |      |      |                                                                                                                 |
| **RefCOCO+**                   |         20K(Images)         | 142K  |  142K  |                                                                                 [Modeling Context in Referring Expressions](https://arxiv.org/pdf/1608.00272v3)                                                                                  |      |      |                                                                                                                 |
| **RefCOCOg**                   |        26.7K(Images)        | 85.5K | 85.5K  |                                                                                 [Modeling Context in Referring Expressions](https://arxiv.org/pdf/1608.00272v3)                                                                                  |      |      |                                                                                                                 |
| **TextVQA**                    |        28.4(Images)         | 45.3K | 45.3K  |                                              [Towards VQA Models That Can Read](https://openaccess.thecvf.com/content_CVPR_2019/papers/Singh_Towards_VQA_Models_That_Can_Read_CVPR_2019_paper.pdf)                                               |      |      |                                                                                                                 |
| **DocVQA**                     |         12K(Images)         |  50K  |  50K   |                                                                                 [DocVQA:A Dataset for VQA on Document Images](https://arxiv.org/pdf/2007.00398)                                                                                  |      |      |                                                                                                                 |
| **ST-VQA**                     |         23K(Images)         |  32K  |  32K   |                                          [Scene Text Visual Question Answering](https://openaccess.thecvf.com/content_ICCV_2019/papers/Biten_Scene_Text_Visual_Question_Answering_ICCV_2019_paper.pdf)                                           |      |      |                                                                                                                 |
| **A-OKVQA**                    |        23.7K(Images)        | 24.9K | 24.9K  |                                                [A-OKVQA: A Benchmark for Visual Question Answering using World Knowledge](https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136680141.pdf)                                                |      |      |                                                                                                                 |
| **Multimodal ArxivQA**         |         32K(Images)         | 16.6K | 16.6K  |                                                      [Multimodal ArXiv: A Dataset for Improving Scientific Comprehension of Large Vision-Language Models](https://arxiv.org/pdf/2403.00231)                                                      |      |      |                                                                                                                 |
| **OK-VQA**                     |         14K(Images)         |  14K  |  14K   |    [OK-VQA: A Visual Question Answering Benchmark Requiring External Knowledge](https://openaccess.thecvf.com/content_CVPR_2019/papers/Marino_OK-VQA_A_Visual_Question_Answering_Benchmark_Requiring_External_Knowledge_CVPR_2019_paper.pdf)     |      |      |                                                                                                                 |
| **WebVid**                     |         10M(Video)          |  10M  |  10M   |                  [Frozen in Time: A Joint Video and Image Encoder for End-to-End Retrieval](https://openaccess.thecvf.com/content/ICCV2021/papers/Bain_Frozen_in_Time_A_Joint_Video_and_Image_Encoder_for_ICCV_2021_paper.pdf)                   |      |      |                                                                                                                 |
| **MSRVTT**                     |         10K(Video)          | 200K  |  200K  |                                   [MSR-VTT: A Large Video Description Dataset for Bridging Video and Language](https://openaccess.thecvf.com/content_cvpr_2016/papers/Xu_MSR-VTT_A_Large_CVPR_2016_paper.pdf)                                    |      |      |                                                                                                                 |
| **YFCC100M**                   | 99.2M(Images), 0.8M(Videos) |   -   |   -    |                                                                               [YFCC100M: The New Data in Multimedia Research](https://arxiv.org/pdf/1503.01817v2)                                                                                |      |      | a total of 100 million media objects, of which approximately 99.2 million are photos and 0.8 million are videos |
| **VSDial-CN**                  |  120K (Image), 1.2M(Audio)  | 120K  |  1.2M  |                                                        [VILAS: EXPLORING THE EFFECTS OF VISION AND LANGUAGE CONTEXT IN AUTOMATIC SPEECH RECOGNITION](https://arxiv.org/pdf/2305.19972v2)                                                         |      |      |                                                                                                                 |
| **AISHELL-2**                  |              -              |   -   |   1M   |                                                                    [AISHELL-2: Transforming Mandarin ASR Research Into Industrial Scale](https://arxiv.org/pdf/1808.10583v2)                                                                     |      |      |                                                                                                                 |
| **AISHELL-1**                  |              -              |   -   |  128K  |                                                             [AISHELL-1: AN OPEN-SOURCE MANDARIN SPEECH CORPUS AND A SPEECH RECOGNITION BASELINE](https://arxiv.org/pdf/1709.05522v1)                                                             |      |      |                                                                                                                 |
| **WavCaps**                    |         403K(Audio)         | 403K  |  403K  |                                                [WavCaps: A ChatGPT-Assisted Weakly-Labelled Audio Captioning Dataset for Audio-Language Multimodal Research](https://arxiv.org/pdf/2303.17395v2)                                                 |      |      |                                                                                                                 |


## Datasets of Multimodal Instruction Tuning

| Name                    | Type | I->O   | Method | #.Instance |                                                                               Paper                                                                               |                                              Link                                               |                                                                                                            Notes                                                                                                             |
|:------------------------|------|--------|--------|------------|:-----------------------------------------------------------------------------------------------------------------------------------------------------------------:|:-----------------------------------------------------------------------------------------------:|:----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------:|
| **MiniGPT-4's IT**      | SFT  | I+T->T | Auto.  | 5K         |                                                                                                                                                                   |                                                                                                 |                                                                                                                                                                                                                              |
| **UNK-VQA**             |      |        |        |            |                    [UNK-VQA: A Dataset and a Probe into the Abstention Ability of Multi-modal Large Models](https://arxiv.org/pdf/2310.10942)                     |                           [Link](https://github.com/guoyang9/UNK-VQA)                           |                                                                     A dataset designed to teach models to refrain from answering unanswerable questions                                                                      |
| **VEGA**                |      |        |        |            |                      [VEGA: Learning Interleaved Image-Text Comprehension in Vision-Language Large Models](https://arxiv.org/pdf/2406.10228)                      |                             [Link](https://github.com/zhourax/VEGA)                             |                                                                    A dataset for enhancing model capabilities in comprehension of interleaved information                                                                    | 
| **ALLaVA-4V**           |      |        |        |            |                        [ALLaVA: Harnessing GPT4V-synthesized Data for A Lite Vision-Language Model](https://arxiv.org/pdf/2402.11684.pdf)                         |              [Link](https://huggingface.co/datasets/FreedomIntelligence/ALLaVA-4V)              |                                                                            Vision and language caption and instruction dataset generated by GPT4V                                                                            | 
| **IDK**                 |      |        |        |            |                         [Visually Dehallucinative Instruction Generation: Know What You Don't Know](https://arxiv.org/pdf/2402.09717.pdf)                         |                              [Link](https://github.com/ncsoft/idk)                              |                                                                                Dehallucinative visual instruction for "I Know" hallucination                                                                                 |
| **CAP2QA**              |      |        |        |            |                                      [Visually Dehallucinative Instruction Generation](https://arxiv.org/pdf/2402.08348.pdf)                                      |                            [Link](https://github.com/ncsoft/cap2qa)                             |                                                                                           Image-aligned visual instruction dataset                                                                                           |
| **M3DBench**            |      |        |        |            |                             [M3DBench: Let's Instruct Large Models with Multi-modal 3D Prompts](https://arxiv.org/pdf/2312.10763.pdf)                             |                           [Link](https://github.com/OpenM3D/M3DBench)                           |                                                                                         A large-scale 3D instruction tuning dataset                                                                                          |
| **ViP-LLaVA-Instruct**  |      |        |        |            |                            [Making Large Multimodal Models Understand Arbitrary Visual Prompts](https://arxiv.org/pdf/2312.00784.pdf)                             |                [Link](https://huggingface.co/datasets/mucai/ViP-LLaVA-Instruct)                 |                                                                      A mixture of LLaVA-1.5 instruction data and the region-level visual prompting data                                                                      |
| **LVIS-Instruct4V**     |      |        |        |            |                        [To See is to Believe: Prompting GPT-4V for Better Visual Instruction Tuning](https://arxiv.org/pdf/2311.07574.pdf)                        |                  [Link](https://huggingface.co/datasets/X2FD/LVIS-Instruct4V)                   |                                                                                A visual instruction dataset via self-instruction from GPT-4V                                                                                 |
| **ComVint**             |      |        |        |            | [What Makes for Good Visual Instructions? Synthesizing Complex Visual Reasoning Instructions for Visual Instruction Tuning](https://arxiv.org/pdf/2311.01487.pdf) |                    [Link](https://github.com/RUCAIBox/ComVint#comvint-data)                     |                                                                                 A synthetic instruction dataset for complex visual reasoning                                                                                 |
| **SparklesDialogue**    |      |        |        |            |               [âœ¨Sparkles: Unlocking Chats Across Multiple Images for Multimodal Instruction-Following Models](https://arxiv.org/pdf/2308.16463.pdf)               |                  [Link](https://github.com/HYPJUDY/Sparkles#sparklesdialogue)                   | A machine-generated dialogue dataset tailored for word-level interleaved multi-image and text interactions to augment the conversational competence of instruction-following LLMs across multiple images and dialogue turns. |
| **StableLLaVA**         |      |        |        |            |                  [StableLLaVA: Enhanced Visual Instruction Tuning with Synthesized Image-Dialogue Data](https://arxiv.org/pdf/2308.10253v1.pdf)                   |                          [Link](https://github.com/icoz69/StableLLAVA)                          |                                                                           A cheap and effective approach to collect visual instruction tuning data                                                                           |
| **M-HalDetect**         |      |        |        |            |                          [Detecting and Preventing Hallucinations in Large Vision Language Models](https://arxiv.org/pdf/2308.06394.pdf)                          |                                         [Coming soon]()                                         |                                                                   A dataset used to train and benchmark models for hallucination detection and prevention                                                                    | 
| **MGVLID**              |      |        |        |            |                     [ChatSpot: Bootstrapping Multimodal LLMs via Precise Referring Instruction Tuning](https://arxiv.org/pdf/2307.09474.pdf)                      |                                                -                                                |                                                                     A high-quality instruction-tuning dataset including image-text and region-text pairs                                                                     |
| **BuboGPT**             |      |        |        |            |                                  [BuboGPT: Enabling Visual Grounding in Multi-Modal LLMs](https://arxiv.org/pdf/2307.08581.pdf)                                   |                     [Link](https://huggingface.co/datasets/magicr/BuboGPT)                      |                                                   A high-quality instruction-tuning dataset including audio-text audio caption data and audio-image-text localization data                                                   |
| **SVIT**                |      |        |        |            |                                        [SVIT: Scaling up Visual Instruction Tuning](https://arxiv.org/pdf/2307.04087.pdf)                                         |                        [Link](https://huggingface.co/datasets/BAAI/SVIT)                        |                               A large-scale dataset with 4.2M informative visual instruction tuning data, including conversations, detailed descriptions, complex reasoning and referring QAs                                |
| **mPLUG-DocOwl**        |      |        |        |            |                   [mPLUG-DocOwl: Modularized Multimodal Large Language Model for Document Understanding](https://arxiv.org/pdf/2307.02499.pdf)                    |                 [Link](https://github.com/X-PLUG/mPLUG-DocOwl/tree/main/DocLLM)                 |                                              An instruction tuning dataset featuring a wide range of visual-text understanding tasks including OCR-free document understanding                                               | 
| **PF-1M**               |      |        |        |            |                                      [Visual Instruction Tuning with Polite Flamingo](https://arxiv.org/pdf/2307.01003.pdf)                                       |               [Link](https://huggingface.co/datasets/chendelong/PF-1M/tree/main)                |                                                                   A collection of 37 vision-language datasets with responses rewritten by Polite Flamingo.                                                                   | 
| **ChartLlama**          |      |        |        |            |                            [ChartLlama: A Multimodal LLM for Chart Understanding and Generation](https://arxiv.org/pdf/2311.16483.pdf)                            |            [Link](https://huggingface.co/datasets/listen2you002/ChartLlama-Dataset)             |                                                                       A multi-modal instruction-tuning dataset for chart understanding and generation                                                                        |
| **LLaVAR**              |      |        |        |            |                       [LLaVAR: Enhanced Visual Instruction Tuning for Text-Rich Image Understanding](https://arxiv.org/pdf/2306.17107.pdf)                        |                             [Link](https://llavar.github.io/#data)                              |                                                                            A visual instruction-tuning dataset for Text-rich Image Understanding                                                                             | 
| **MotionGPT**           |      |        |        |            |                                       [MotionGPT: Human Motion as a Foreign Language](https://arxiv.org/pdf/2306.14795.pdf)                                       |                       [Link](https://github.com/OpenMotionLab/MotionGPT)                        |                                                                          A instruction-tuning dataset including multiple human motion-related tasks                                                                          |
| **LRV-Instruction**     |      |        |        |            |                    [Mitigating Hallucination in Large Multi-Modal Models via Robust Instruction Tuning](https://arxiv.org/pdf/2306.14565.pdf)                     |  [Link](https://github.com/FuxiaoLiu/LRV-Instruction#visual-instruction-data-lrv-instruction)   |                                                                             Visual instruction tuning dataset for addressing hallucination issue                                                                             | 
| **Macaw-LLM**           |      |        |        |            |                  [Macaw-LLM: Multi-Modal Language Modeling with Image, Audio, Video, and Text Integration](https://arxiv.org/pdf/2306.09093.pdf)                  |                 [Link](https://github.com/lyuchenyang/Macaw-LLM/tree/main/data)                 |                                                                        A large-scale multi-modal instruction dataset in terms of multi-turn dialogue                                                                         | 
| **LAMM-Dataset**        |      |        |        |            |                 [LAMM: Language-Assisted Multi-Modal Instruction-Tuning Dataset, Framework, and Benchmark](https://arxiv.org/pdf/2306.06687.pdf)                  |                      [Link](https://github.com/OpenLAMM/LAMM#lamm-dataset)                      |                                                                                    A comprehensive multi-modal instruction tuning dataset                                                                                    |
| **Video-ChatGPT**       |      |        |        |            |                 [Video-ChatGPT: Towards Detailed Video Understanding via Large Vision and Language Models](https://arxiv.org/pdf/2306.05424.pdf)                  | [Link](https://github.com/mbzuai-oryx/Video-ChatGPT#video-instruction-dataset-open_file_folder) |                                                                                         100K high-quality video instruction dataset                                                                                          | 
| **MIMIC-IT**            |      |        |        |            |                                    [MIMIC-IT: Multi-Modal In-Context Instruction Tuning](https://arxiv.org/pdf/2306.05425.pdf)                                    |              [Link](https://github.com/Luodian/Otter/blob/main/mimic-it/README.md)              |                                                                                           Multimodal in-context instruction tuning                                                                                           |
| **M<sup>3</sup>IT**     |      |        |        |            |                [M<sup>3</sup>IT: A Large-Scale Dataset towards Multi-Modal Multilingual Instruction Tuning](https://arxiv.org/pdf/2306.04387.pdf)                 |                   [Link](https://huggingface.co/datasets/MMInstruction/M3IT)                    |                                                                              Large-scale, broad-coverage multimodal instruction tuning dataset                                                                               | 
| **LLaVA-Med**           |      |        |        |            |                   [LLaVA-Med: Training a Large Language-and-Vision Assistant for Biomedicine in One Day](https://arxiv.org/pdf/2306.00890.pdf)                    |             [Coming soon](https://github.com/microsoft/LLaVA-Med#llava-med-dataset)             |                                                                            A large-scale, broad-coverage biomedical instruction-following dataset                                                                            |
| **GPT4Tools**           |      |        |        |            |                        [GPT4Tools: Teaching Large Language Model to Use Tools via Self-instruction](https://arxiv.org/pdf/2305.18752.pdf)                         |                    [Link](https://github.com/StevenGrove/GPT4Tools#dataset)                     |                                                                                              Tool-related instruction datasets                                                                                               |
| **MULTIS**              |      |        |        |            |                     [ChatBridge: Bridging Modalities with Large Language Model as a Language Catalyst](https://arxiv.org/pdf/2305.16103.pdf)                      |                        [Coming soon](https://iva-chatbridge.github.io/)                         |                                                                              Multimodal instruction tuning dataset covering 16 multimodal tasks                                                                              |
| **DetGPT**              |      |        |        |            |                                        [DetGPT: Detect What You Need via Reasoning](https://arxiv.org/pdf/2305.14167.pdf)                                         |                [Link](https://github.com/OptimalScale/DetGPT/tree/main/dataset)                 |                                                                       Instruction-tuning dataset with 5000 images and around 30000 query-answer pairs                                                                        |
| **PMC-VQA**             |      |        |        |            |                         [PMC-VQA: Visual Instruction Tuning for Medical Visual Question Answering](https://arxiv.org/pdf/2305.10415.pdf)                          |                     [Coming soon](https://xiaoman-zhang.github.io/PMC-VQA/)                     |                                                                                    Large-scale medical visual question-answering dataset                                                                                     |
| **VideoChat**           |      |        |        |            |                                        [VideoChat: Chat-Centric Video Understanding](https://arxiv.org/pdf/2305.06355.pdf)                                        |        [Link](https://github.com/OpenGVLab/InternVideo/tree/main/Data/instruction_data)         |                                                                                         Video-centric multimodal instruction dataset                                                                                         |
| **X-LLM**               |      |        |        |            |           [X-LLM: Bootstrapping Advanced Large Language Models by Treating Multi-Modalities as Foreign Languages](https://arxiv.org/pdf/2305.04160.pdf)           |                          [Link](https://github.com/phellonchen/X-LLM)                           |                                                                                            Chinese multimodal instruction dataset                                                                                            |
| **LMEye**               |      |        |        |            |                            [LMEye: An Interactive Perception Network for Large Language Models](https://arxiv.org/pdf/2305.03701.pdf)                             |         [Link](https://huggingface.co/datasets/YunxinLi/Multimodal_Insturction_Data_V2)         |                                                                                           A multi-modal instruction-tuning dataset                                                                                           |
| **cc-sbu-align**        |      |        |        |            |                  [MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models](https://arxiv.org/pdf/2304.10592.pdf)                   |                [Link](https://huggingface.co/datasets/Vision-CAIR/cc_sbu_align)                 |                                                                     Multimodal aligned dataset for improving model's usability and generation's fluency                                                                      |
| **LLaVA-Instruct-150K** |      |        |        |            |                                                 [Visual Instruction Tuning](https://arxiv.org/pdf/2304.08485.pdf)                                                 |             [Link](https://huggingface.co/datasets/liuhaotian/LLaVA-Instruct-150K)              |                                                                                    Multimodal instruction-following data generated by GPT                                                                                    |
| **MultiInstruct**       |      |        |        |            |                      [MultiInstruct: Improving Multi-Modal Zero-Shot Learning via Instruction Tuning](https://arxiv.org/pdf/2212.10773.pdf)                       |                         [Link](https://github.com/VT-NLP/MultiInstruct)                         |                                                                                  The first multimodal instruction tuning benchmark dataset                                                                                   |


## Datasets of In-Context Learning

| Name         |                                                        Paper                                                         |                                 Link                                  |                                                                                   Notes                                                                                    |
|:-------------|:--------------------------------------------------------------------------------------------------------------------:|:---------------------------------------------------------------------:|:--------------------------------------------------------------------------------------------------------------------------------------------------------------------------:|
| **MIC**      | [MMICL: Empowering Vision-language Model with Multi-Modal In-Context Learning](https://arxiv.org/pdf/2309.07915.pdf) |      [Link](https://huggingface.co/datasets/BleachNick/MIC_full)      | A manually constructed instruction tuning dataset including interleaved text-image inputs, inter-related multiple image inputs, and multimodal in-context learning inputs. |
| **MIMIC-IT** |             [MIMIC-IT: Multi-Modal In-Context Instruction Tuning](https://arxiv.org/pdf/2306.05425.pdf)              | [Link](https://github.com/Luodian/Otter/blob/main/mimic-it/README.md) |                                                                 Multimodal in-context instruction dataset                                                                  |

## Datasets of Multimodal Chain-of-Thought

| Name          |                                                                                                    Paper                                                                                                     |                                         Link                                          |                                            Notes                                             |
|:--------------|:------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------:|:-------------------------------------------------------------------------------------:|:--------------------------------------------------------------------------------------------:|
| **EMER**      |                                                               [Explainable Multimodal Emotion Reasoning](https://arxiv.org/pdf/2306.15401.pdf)                                                               | [Coming soon](https://github.com/zeroQiaoba/Explainable-Multimodal-Emotion-Reasoning) |                  A benchmark dataset for explainable emotion reasoning task                  |
| **EgoCOT**    |                                               [EmbodiedGPT: Vision-Language Pre-Training via Embodied Chain of Thought](https://arxiv.org/pdf/2305.15021.pdf)                                                |           [Coming soon](https://github.com/EmbodiedGPT/EmbodiedGPT_Pytorch)           |                            Large-scale embodied planning dataset                             |
| **VIP**       |                                  [Letâ€™s Think Frame by Frame: Evaluating Video Chain of Thought with Video Infilling and Prediction](https://arxiv.org/pdf/2305.13903.pdf)                                   |                                    [Coming soon]()                                    |               An inference-time dataset that can be used to evaluate VideoCOT                |
| **ScienceQA** | [Learn to Explain: Multimodal Reasoning via Thought Chains for Science Question Answering](https://proceedings.neurips.cc/paper_files/paper/2022/file/11332b6b6cf4485b84afadb1352d3a9a-Paper-Conference.pdf) |       [Link](https://github.com/lupantech/ScienceQA#ghost-download-the-dataset)       | Large-scale multi-choice dataset, featuring multimodal science questions and diverse domains | 

## Datasets of Multimodal RLHF

| Name           |                                                  Paper                                                   |                               Link                               |                       Notes                        |
|:---------------|:--------------------------------------------------------------------------------------------------------:|:----------------------------------------------------------------:|:--------------------------------------------------:|
| **VLFeedback** | [Silkie: Preference Distillation for Large Visual Language Models](https://arxiv.org/pdf/2312.10665.pdf) | [Link](https://huggingface.co/datasets/MMInstruction/VLFeedback) | A vision-language feedback dataset annotated by AI |

## Benchmarks for Evaluation

| Name                         |                                                                                                           Paper                                                                                                           |                                            Link                                             |                                                                                 Notes                                                                                 |
|:-----------------------------|:-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------:|:-------------------------------------------------------------------------------------------:|:---------------------------------------------------------------------------------------------------------------------------------------------------------------------:|
| **MME-RealWorld**            |                                [MME-RealWorld: Could Your Multimodal LLM Challenge High-Resolution Real-World Scenarios that are Difficult for Humans?](https://arxiv.org/pdf/2408.13257)                                 |             [Link](https://huggingface.co/datasets/yifanzhang114/MME-RealWorld)             |                                                       A challenging benchmark that involves real-life scenarios                                                       |
| **CharXiv**                  |                                                      [CharXiv: Charting Gaps in Realistic Chart Understanding in Multimodal LLMs](https://arxiv.org/pdf/2406.18521)                                                       |                [Link](https://huggingface.co/datasets/princeton-nlp/CharXiv)                |                                                        Chart understanding benchmark curated by human experts                                                         |
| **Video-MME**                |                                          [Video-MME: The First-Ever Comprehensive Evaluation Benchmark of Multi-modal LLMs in Video Analysis](https://arxiv.org/pdf/2405.21075)                                           |                        [Link](https://github.com/BradyFU/Video-MME)                         |                                              A comprehensive evaluation benchmark of Multi-modal LLMs in video analysis                                               |
| **VL-ICL Bench**             |                                               [VL-ICL Bench: The Devil in the Details of Benchmarking Multimodal In-Context Learning](https://arxiv.org/pdf/2403.13164.pdf)                                               |                          [Link](https://github.com/ys-zong/VL-ICL)                          |                                                  A benchmark for M-ICL evaluation, covering a wide spectrum of tasks                                                  |
| **TempCompass**              |                                                               [TempCompass: Do Video LLMs Really Understand Videos?](https://arxiv.org/pdf/2403.00476.pdf)                                                                |                        [Link](https://github.com/llyx97/TempCompass)                        |                                                 A benchmark to evaluate the temporal perception ability of Video LLMs                                                 |
| **CoBSAT**                   |                                                               [Can MLLMs Perform Text-to-Image In-Context Learning?](https://arxiv.org/pdf/2402.01293.pdf)                                                                |                   [Link](https://huggingface.co/datasets/yzeng58/CoBSAT)                    |                                                                   A benchmark for text-to-image ICL                                                                   |
| **VQAv2-IDK**                |                                                     [Visually Dehallucinative Instruction Generation: Know What You Don't Know](https://arxiv.org/pdf/2402.09717.pdf)                                                     |                            [Link](https://github.com/ncsoft/idk)                            |                                                        A benchmark for assessing "I Know" visual hallucination                                                        |
| **Math-Vision**              |                                                       [Measuring Multimodal Mathematical Reasoning with MATH-Vision Dataset](https://arxiv.org/pdf/2402.14804.pdf)                                                        |                    [Link](https://github.com/mathvision-cuhk/MathVision)                    |                                                              A diverse mathematical reasoning benchmark                                                               |
| **CMMMU**                    |                                                   [CMMMU: A Chinese Massive Multi-discipline Multimodal Understanding Benchmark](https://arxiv.org/pdf/2401.11944.pdf)                                                    |                      [Link](https://github.com/CMMMU-Benchmark/CMMMU)                       |                                           A Chinese benchmark involving reasoning and knowledge across multiple disciplines                                           |
| **MMCBench**                 |                                                          [Benchmarking Large Multimodal Models against Common Corruptions](https://arxiv.org/pdf/2401.11943.pdf)                                                          |                         [Link](https://github.com/sail-sg/MMCBench)                         |                                                  A benchmark for examining self-consistency under common corruptions                                                  |
| **MMVP**                     |                                                       [Eyes Wide Shut? Exploring the Visual Shortcomings of Multimodal LLMs](https://arxiv.org/pdf/2401.06209.pdf)                                                        |                           [Link](https://github.com/tsb0601/MMVP)                           |                                                             A benchmark for assessing visual capabilities                                                             |
| **TimeIT**                   |                                              [TimeChat: A Time-sensitive Multimodal Large Language Model for Long Video Understanding](https://arxiv.org/pdf/2312.02051.pdf)                                              |                  [Link](https://huggingface.co/datasets/ShuhuaiRen/TimeIT)                  |                       A video instruction-tuning dataset with timestamp annotations, covering diverse time-sensitive video-understanding tasks.                       |
| **ViP-Bench**                |                                                        [Making Large Multimodal Models Understand Arbitrary Visual Prompts](https://arxiv.org/pdf/2312.00784.pdf)                                                         |                   [Link](https://huggingface.co/datasets/mucai/ViP-Bench)                   |                                                                    A benchmark for visual prompts                                                                     |
| **M3DBench**                 |                                                         [M3DBench: Let's Instruct Large Models with Multi-modal 3D Prompts](https://arxiv.org/pdf/2312.10763.pdf)                                                         |                         [Link](https://github.com/OpenM3D/M3DBench)                         |                                                                        A 3D-centric benchmark                                                                         |
| **Video-Bench**              |                                        [Video-Bench: A Comprehensive Benchmark and Toolkit for Evaluating Video-based Large Language Models](https://arxiv.org/pdf/2311.16103.pdf)                                        |                    [Link](https://github.com/PKU-YuanGroup/Video-Bench)                     |                                                                 A benchmark for video-MLLM evaluation                                                                 |
| **Charting-New-Territories** |                                         [Charting New Territories: Exploring the Geographic and Geospatial Capabilities of Multimodal LLMs](https://arxiv.org/pdf/2311.14656.pdf)                                         |            [Link](https://github.com/jonathan-roberts1/charting-new-territories)            |                                                   A benchmark for evaluating geographic and geospatial capabilities                                                   |
| **MLLM-Bench**               |                                                               [MLLM-Bench, Evaluating Multi-modal LLMs using GPT-4V](https://arxiv.org/pdf/2311.13951.pdf)                                                                |                  [Link](https://github.com/FreedomIntelligence/MLLM-Bench)                  |                                                              GPT-4V evaluation with per-sample criteria                                                               |
| **BenchLMM**                 |                                                  [BenchLMM: Benchmarking Cross-style Visual Capability of Large Multimodal Models](https://arxiv.org/pdf/2312.02896.pdf)                                                  |                   [Link](https://huggingface.co/datasets/AIFEG/BenchLMM)                    |                                              A benchmark for assessment of the robustness against different image styles                                              |
| **MMC-Benchmark**            |                                                 [MMC: Advancing Multimodal Chart Understanding with Large-scale Instruction Tuning](https://arxiv.org/pdf/2311.10774.pdf)                                                 |                          [Link](https://github.com/FuxiaoLiu/MMC)                           |                              A comprehensive human-annotated benchmark with distinct tasks evaluating reasoning capabilities over charts                              |
| **MVBench**                  |                                                        [MVBench: A Comprehensive Multi-modal Video Understanding Benchmark](https://arxiv.org/pdf/2311.17005.pdf)                                                         |     [Link](https://github.com/OpenGVLab/Ask-Anything/blob/main/video_chat2/MVBENCH.md)      |                                                     A comprehensive multimodal benchmark for video understanding                                                      |
| **Bingo**                    |                                               [Holistic Analysis of Hallucination in GPT-4V(ision): Bias and Interference Challenges](https://arxiv.org/pdf/2311.03287.pdf)                                               |                           [Link](https://github.com/gzcch/Bingo)                            |                                               A benchmark for hallucination evaluation that focuses on two common types                                               |
| **MagnifierBench**           |                                                                  [OtterHD: A High-Resolution Multi-modality Model](https://arxiv.org/pdf/2311.04219.pdf)                                                                  |               [Link](https://huggingface.co/datasets/Otter-AI/MagnifierBench)               |                                               A benchmark designed to probe models' ability of fine-grained perception                                                |
| **HallusionBench**           | [HallusionBench: You See What You Think? Or You Think What You See? An Image-Context Reasoning Benchmark Challenging for GPT-4V(ision), LLaVA-1.5, and Other Multi-modality Models](https://arxiv.org/pdf/2310.14566.pdf) |                    [Link](https://github.com/tianyi-lab/HallusionBench)                     |                                                 An image-context reasoning benchmark for evaluation of hallucination                                                  |
| **PCA-EVAL**                 |                            [Towards End-to-End Embodied Decision Making via Multi-modal Large Language Model: Explorations with GPT4-Vision and Beyond](https://arxiv.org/pdf/2310.02071.pdf)                             |                      [Link](https://github.com/pkunlp-icler/PCA-EVAL)                       |                                                   A benchmark for evaluating multi-domain embodied decision-making.                                                   |
| **MMHal-Bench**              |                                                          [Aligning Large Multimodal Models with Factually Augmented RLHF](https://arxiv.org/pdf/2309.14525.pdf)                                                           |              [Link](https://huggingface.co/datasets/Shengcao1006/MMHal-Bench)               |                                                               A benchmark for hallucination evaluation                                                                |
| **MathVista**                |                                   [MathVista: Evaluating Math Reasoning in Visual Contexts with GPT-4V, Bard, and Other Large Multimodal Models](https://arxiv.org/pdf/2310.02255.pdf)                                    |                  [Link](https://huggingface.co/datasets/AI4Math/MathVista)                  |                                                A benchmark that challenges both visual and math reasoning capabilities                                                |
| **SparklesEval**             |                                           [âœ¨Sparkles: Unlocking Chats Across Multiple Images for Multimodal Instruction-Following Models](https://arxiv.org/pdf/2308.16463.pdf)                                           |                  [Link](https://github.com/HYPJUDY/Sparkles#sparkleseval)                   | A GPT-assisted benchmark for quantitatively assessing a model's conversational competence across multiple images and dialogue turns based on three distinct criteria. |
| **ISEKAI**                   |                                                                     [Link-Context Learning for Multimodal LLMs](https://arxiv.org/pdf/2308.07891.pdf)                                                                     |                        [Link](https://huggingface.co/ISEKAI-Portal)                         |                              A benchmark comprising exclusively of unseen generated image-label pairs designed for link-context learning                              |
| **M-HalDetect**              |                                                      [Detecting and Preventing Hallucinations in Large Vision Language Models](https://arxiv.org/pdf/2308.06394.pdf)                                                      |                                       [Coming soon]()                                       |                                        A dataset used to train and benchmark models for hallucination detection and prevention                                        | 
| **I4**                       |                                               [Empowering Vision-Language Models to Follow Interleaved Vision-Language Instructions](https://arxiv.org/pdf/2308.04152.pdf)                                                |                         [Link](https://github.com/DCDmllm/Cheetah)                          |                   A benchmark to comprehensively evaluate the instruction following ability on complicated interleaved vision-language instructions                   | 
| **SciGraphQA**               |                                          [SciGraphQA: A Large-Scale Synthetic Multi-Turn Question-Answering Dataset for Scientific Graphs](https://arxiv.org/pdf/2308.03349.pdf)                                          |                    [Link](https://github.com/findalexli/SciGraphQA#data)                    |                                                         A large-scale chart-visual question-answering dataset                                                         |
| **MM-Vet**                   |                                                      [MM-Vet: Evaluating Large Multimodal Models for Integrated Capabilities](https://arxiv.org/pdf/2308.02490.pdf)                                                       |                         [Link](https://github.com/yuweihao/MM-Vet)                          |                                     An evaluation benchmark that examines large multimodal models on complicated multimodal tasks                                     |
| **SEED-Bench**               |                                                      [SEED-Bench: Benchmarking Multimodal LLMs with Generative Comprehension](https://arxiv.org/pdf/2307.16125.pdf)                                                       |                       [Link](https://github.com/AILab-CVC/SEED-Bench)                       |                                                    A benchmark for evaluation of generative comprehension in MLLMs                                                    | 
| **MMBench**                  |                                                             [MMBench: Is Your Multi-modal Model an All-around Player?](https://arxiv.org/pdf/2307.06281.pdf)                                                              |                       [Link](https://github.com/open-compass/MMBench)                       |                         A systematically-designed objective benchmark for robustly evaluating the various abilities of vision-language models                         |
| **Lynx**                     |                                                   [What Matters in Training a GPT4-Style Language Model with Multimodal Inputs?](https://arxiv.org/pdf/2307.02469.pdf)                                                    |                 [Link](https://github.com/bytedance/lynx-llm#prepare-data)                  |                                               A comprehensive evaluation benchmark including both image and video tasks                                               |
| **GAVIE**                    |                                                [Mitigating Hallucination in Large Multi-Modal Models via Robust Instruction Tuning](https://arxiv.org/pdf/2306.14565.pdf)                                                 |            [Link](https://github.com/FuxiaoLiu/LRV-Instruction#evaluationgavie)             |                                              A benchmark to evaluate the hallucination and instruction following ability                                              | 
| **MME**                      |                                                  [MME: A Comprehensive Evaluation Benchmark for Multimodal Large Language Models](https://arxiv.org/pdf/2306.13394.pdf)                                                   | [Link](https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models/tree/Evaluation) |                                                               A comprehensive MLLM Evaluation benchmark                                                               |
| **LVLM-eHub**                |                                                 [LVLM-eHub: A Comprehensive Evaluation Benchmark for Large Vision-Language Models](https://arxiv.org/pdf/2306.09265.pdf)                                                  |                  [Link](https://github.com/OpenGVLab/Multi-Modality-Arena)                  |                                                                   An evaluation platform for MLLMs                                                                    |
| **LAMM-Benchmark**           |                                             [LAMM: Language-Assisted Multi-Modal Instruction-Tuning Dataset, Framework, and Benchmark](https://arxiv.org/pdf/2306.06687.pdf)                                              |                   [Link](https://github.com/OpenLAMM/LAMM#lamm-benchmark)                   |                                    A benchmark for evaluating  the quantitative performance of MLLMs on various2D/3D vision tasks                                     |
| **M3Exam**                   |                                           [M3Exam: A Multilingual, Multimodal, Multilevel Benchmark for Examining Large Language Models](https://arxiv.org/pdf/2306.05179.pdf)                                            |                        [Link](https://github.com/DAMO-NLP-SG/M3Exam)                        |                                                 A multilingual, multimodal, multilevel benchmark for evaluating MLLM                                                  |
| **OwlEval**                  |                                                    [mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality](https://arxiv.org/pdf/2304.14178.pdf)                                                    |                [Link](https://github.com/X-PLUG/mPLUG-Owl/tree/main/OwlEval)                |                                                            Dataset for evaluation on multiple capabilities                                                            |
