# Awesome-MLLM-Datasets
ðŸš€ðŸš€ðŸš€This project aims to collect and collate various datasets for multimodal large model training, including but not limited to pre-training data, instruction fine-tuning data, and In-Context learning data.

ðŸ’¡ðŸ’¡ðŸ’¡The goal of the project is to provide researchers with a comprehensive repository of resources to support their ability to more easily access high-quality datasets when developing and optimizing multimodal AI systems.

**Table of Contents**
- [Datasets of Pre-Training](#datasets-of-pre-training)
- [Datasets of Multimodal Instruction Tuning](#datasets-of-multimodal-instruction-tuning)
- [Datasets of In-Context Learning](#datasets-of-in-context-learning)
- [Datasets of Multimodal Chain-of-Thought](#datasets-of-multimodal-chain-of-thought)
- [Datasets of Multimodal RLHF](#datasets-of-multimodal-rlhf)
- [Benchmarks for Evaluation](#benchmarks-for-evaluation)

## Datasets of Pre-Training

|          Dataset           |             #.X             |  #.T  | #.X-T  |                                                                                                                      Paper                                                                                                                       | Link |                                                   Description                                                   |
|:--------------------------:|:---------------------------:|:-----:|:------:|:------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------:|:----:|:---------------------------------------------------------------------------------------------------------------:|
|           WebLI            |         10B(Images)         |  12B  |  12B   |                                                                           [PaLI: A Jointly-Scaled Multilingual Language-Image Model](https://arxiv.org/pdf/2209.06794)                                                                           |      |                                                                                                                 |
|          LAION-5B          |        5.9B(Images)         | 5.9B  |  5.9B  |                                                            [LAION-5B: An open large-scale dataset for training next generation image-text models](https://arxiv.org/pdf/2210.08402v1)                                                            |      |                                                                                                                 |
|          LAION-en          |        2.3B(Images)         | 2.3B  |  2.3B  |                                                            [LAION-5B: An open large-scale dataset for training next generation image-text models](https://arxiv.org/pdf/2210.08402v1)                                                            |      |                                                                                                                 |
|           ALIGN            |        1.8B(Images)         | 1.8B  |  1.8B  |                                                         [Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision](https://arxiv.org/pdf/2102.05918v2)                                                          |      |                                                                                                                 |
|          DataComp          |        1.4B(Images)         | 1.4B  |  1.4B  |                                                                  [DATACOMP: In search of the next generation of multimodal datasets](https://openreview.net/pdf?id=dVaWCDMBof)                                                                   |      |                                                                                                                 |
|            COYO            |        747M(Images)         | 747M  |  747M  |                                                                           [COYO-700M: Large-scale Image-Text Pair Dataset](https://github.com/kakaobrain/coyo-dataset)                                                                           |      |                                                                                                                 |
|         LAION-COCO         |        600M(Images)         | 600M  |  600M  |                                                                             [LAION COCO: 600M SYNTHETIC CAPTIONS FROM LAION2B-EN](https://laion.ai/blog/laion-coco/)                                                                             |      |                                                                                                                 |
|         LAION-400M         |        400M(Images)         | 400M  |  400M  |                                                                   [LAION-400M: Open Dataset of CLIP-Filtered 400 Million Image-Text Pairs](https://arxiv.org/pdf/2111.02114v1)                                                                   |      |                                                                                                                 |
|       Episodic WebLI       |        400M(Images)         | 400M  |  400M  |                                                                        [PaLI-X: On Scaling up a Multilingual Vision and Language Model](https://arxiv.org/pdf/2305.18565)                                                                        |      |                                                                                                                 |
|            CLIP            |        400M(Images)         | 400M  |  400M  |                                                                   [Learning Transferable Visual Models From Natural Language Supervision](https://arxiv.org/pdf/2106.02524v1)                                                                    |      |                                                                                                                 |
|            LTIP            |        312M(Images)         | 312M  |  312M  |                                                                          [Flamingo: a Visual Language Model for Few-Shot Learning](https://arxiv.org/pdf/2204.14198v2)                                                                           |      |                                                                                                                 |
|           FILIP            |        300M(Images)         | 300M  |  300M  |                                                                        [FILIP: Fine-grained Interactive Language-Image Pre-Training](https://arxiv.org/pdf/2111.07783v1)                                                                         |      |                                                                                                                 | 
|          LAION-zh          |        142M(Images)         | 142M  |  142M  |                                                            [LAION-5B: An open large-scale dataset for training next generation image-text models](https://arxiv.org/pdf/2210.08402v1)                                                            |      |                                                                                                                 |
|    Obelics(Interleaved)    |        353M(Images)         | 115M  |  141M  |                                                           [OBELICS: An Open Web-Scale Filtered Dataset of Interleaved Image-Text Documents](https://openreview.net/pdf?id=SKN2hflBIZ)                                                            |      |                                                                                                                 |
|     MMC4(Interleaved)      |        571M(Images)         |  43B  | 101.2M |                                                                [Multimodal C4: An Open, Billion-scale Corpus of Images Interleaved With Text](https://arxiv.org/pdf/2304.06939v1)                                                                |      |                                                                                                                 |
|           Wukong           |        101M(Images)         | 101M  |  101M  |                                                      [WuKong:100 Million Large-scale Chinese Cross-modal Pre-training Dataset and A Foundation Framework](https://arxiv.org/pdf/2202.06767)                                                      |      |                                                                                                                 |
| M3W(MultiModal MassiveWeb) |        185M(Images)         | 182GB | 43.3M  |                       [Flamingo: a Visual Language Model for Few-Shot Learning](https://storage.googleapis.com/deepmind-media/DeepMind.com/Blog/tackling-multiple-tasks-with-a-single-visual-language-model/flamingo.pdf)                        |      |                                                                                                                 |
|            WIT             |        11.5M(Images)        | 37.6M | 37.6M  |                                                            [WIT: Wikipedia-based Image Text Dataset for Multimodal Multilingual Machine Learning](https://arxiv.org/pdf/2103.01913v2)                                                            |      |                                                                                                                 |
|            GQA             |        113K(Images)         |  22M  |  22M   | [GQA: A New Dataset for Real-World Visual Reasoning and Compositional Question Answering](https://openaccess.thecvf.com/content_CVPR_2019/papers/Hudson_GQA_A_New_Dataset_for_Real-World_Visual_Reasoning_and_Compositional_CVPR_2019_paper.pdf) |      |                                                                                                                 |
|           CC12M            |        12.4M(Images)        | 12.4M | 12.4M  |                                                      [Conceptual 12M: Pushing Web-Scale Image-Text Pre-Training To Recognize Long-Tail Visual Concepts](https://arxiv.org/pdf/2102.08981v2)                                                      |      |                                                                                                                 |
|          Red Caps          |         12M(Images)         |  12M  |  12M   |                                                                 [RedCaps: Web-curated image-text data created by the people, for the people](https://arxiv.org/pdf/2111.11431v1)                                                                 |      |                                                                                                                 |
|       Visual Genome        |        108k(Images)         | 4.5M  |  4.5M  |                                                          [Visual Genome: Connecting Language and Vision Using Crowdsourced Dense Image Annotations](https://arxiv.org/pdf/1602.07332v1)                                                          |      |                                                                                                                 |
|            DVQA            |        300K(Images)         | 3.5M  |  3.5M  |                                    [DVQA: Understanding Data Visualizations via Question Answering](https://openaccess.thecvf.com/content_cvpr_2018/papers/Kafle_DVQA_Understanding_Data_CVPR_2018_paper.pdf)                                    |      |                                                                                                                 |
|            CC3M            |        3.3M(Images)         | 3.3M  |  3.3M  |                                                    [Conceptual Captions: A Cleaned, Hypernymed, Image Alt-text Dataset For Automatic Image Captioning](https://aclanthology.org/P18-1238.pdf)                                                    |      |                                                                                                                 |
|          MS-COCO           |        328k(Images)         | 2.5M  |  2.5M  |                                                                                  [Microsoft COCO: Common Objects in Context](https://arxiv.org/pdf/1405.0312v3)                                                                                  |      |                                                                                                                 |
|   AI Challenger Captions   |        300K(Images)         | 1.5M  |  1.5M  |                                                                [AI Challenger : A Large-scale Dataset for Going Deeper in Image Understanding](https://arxiv.org/pdf/1711.06475)                                                                 |      |                                                                                                                 |
|           VQA v2           |        265K(Images)         | 1.4M  |  1.4M  |                                                      [Making the V in VQA Matter: Elevating the Role of Image Understanding in Visual Question Answering](https://arxiv.org/pdf/1612.00837)                                                      |      |                                                                                                                 |
|     SBU(Image Caption)     |         1M(Images)          |  1M   |   1M   |                                    [Im2Text: Describing Images Using 1 Million Captioned Photographs](https://proceedings.neurips.cc/paper_files/paper/2011/file/5dd9db5e033da9c6fb5ba83c7a7ebea9-Paper.pdf)                                     |      |                                                                                                                 |
|          OCR-VQA           |        207K(Images)         |  1M   |   1M   |                                                             [OCR-VQA: Visual Question Answering by Reading Text in Images](https://anandmishra22.github.io/files/mishra-OCR-VQA.pdf)                                                             |      |                                                                                                                 |
|        COCO Caption        |        164K(Images)         |  1M   |   1M   |                                                                        [Microsoft COCO Captions: Data Collection and Evaluation Server](https://arxiv.org/pdf/1504.00325)                                                                        |      |                                                                                                                 |
|           CC595k           |        595K(Images)         | 595K  |  595K  |                                                                                      [Visual Instruction Tuning](https://openreview.net/pdf?id=w0H2xGHlkw)                                                                                       |      |                                                                                                                 |
|         Visual-7W          |        47.3K(Images)        | 328K  |  328K  |                                                                              [Visual7W: Grounded Question Answering in Images](https://arxiv.org/pdf/1511.03416v4)                                                                               |      |                                                                                                                 |
|         Flickr30k          |         31K(Images)         | 158K  |  158K  |                                          [From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions](https://aclanthology.org/Q14-1006.pdf)                                           |      |                                                                                                                 |
|       Text Captions        |         28K(Images)         | 145K  |  145K  |                                                  [TextCaps: a Dataset for Image Captioning with Reading Comprehension](https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123470732.pdf)                                                   |      |                                                                                                                 |
|          RefCOCO           |         20K(Images)         | 142K  |  142K  |                                                                   [ReferItGame: Referring to Objects in Photographs of Natural Scenes](https://aclanthology.org/D14-1086.pdf)                                                                    |      |                                                                                                                 |
|          RefCOCO+          |         20K(Images)         | 142K  |  142K  |                                                                                 [Modeling Context in Referring Expressions](https://arxiv.org/pdf/1608.00272v3)                                                                                  |      |                                                                                                                 |
|          RefCOCOg          |        26.7K(Images)        | 85.5K | 85.5K  |                                                                                 [Modeling Context in Referring Expressions](https://arxiv.org/pdf/1608.00272v3)                                                                                  |      |                                                                                                                 |
|          TextVQA           |        28.4(Images)         | 45.3K | 45.3K  |                                              [Towards VQA Models That Can Read](https://openaccess.thecvf.com/content_CVPR_2019/papers/Singh_Towards_VQA_Models_That_Can_Read_CVPR_2019_paper.pdf)                                               |      |                                                                                                                 |
|           DocVQA           |         12K(Images)         |  50K  |  50K   |                                                                                 [DocVQA:A Dataset for VQA on Document Images](https://arxiv.org/pdf/2007.00398)                                                                                  |      |                                                                                                                 |
|           ST-VQA           |         23K(Images)         |  32K  |  32K   |                                          [Scene Text Visual Question Answering](https://openaccess.thecvf.com/content_ICCV_2019/papers/Biten_Scene_Text_Visual_Question_Answering_ICCV_2019_paper.pdf)                                           |      |                                                                                                                 |
|          A-OKVQA           |        23.7K(Images)        | 24.9K | 24.9K  |                                                [A-OKVQA: A Benchmark for Visual Question Answering using World Knowledge](https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136680141.pdf)                                                |      |                                                                                                                 |
|     Multimodal ArxivQA     |         32K(Images)         | 16.6K | 16.6K  |                                                      [Multimodal ArXiv: A Dataset for Improving Scientific Comprehension of Large Vision-Language Models](https://arxiv.org/pdf/2403.00231)                                                      |      |                                                                                                                 |
|           OK-VQA           |         14K(Images)         |  14K  |  14K   |    [OK-VQA: A Visual Question Answering Benchmark Requiring External Knowledge](https://openaccess.thecvf.com/content_CVPR_2019/papers/Marino_OK-VQA_A_Visual_Question_Answering_Benchmark_Requiring_External_Knowledge_CVPR_2019_paper.pdf)     |      |                                                                                                                 |
|           WebVid           |         10M(Video)          |  10M  |  10M   |                  [Frozen in Time: A Joint Video and Image Encoder for End-to-End Retrieval](https://openaccess.thecvf.com/content/ICCV2021/papers/Bain_Frozen_in_Time_A_Joint_Video_and_Image_Encoder_for_ICCV_2021_paper.pdf)                   |      |                                                                                                                 |
|           MSRVTT           |         10K(Video)          | 200K  |  200K  |                                   [MSR-VTT: A Large Video Description Dataset for Bridging Video and Language](https://openaccess.thecvf.com/content_cvpr_2016/papers/Xu_MSR-VTT_A_Large_CVPR_2016_paper.pdf)                                    |      |                                                                                                                 |
|          YFCC100M          | 99.2M(Images), 0.8M(Videos) |   -   |   -    |                                                                               [YFCC100M: The New Data in Multimedia Research](https://arxiv.org/pdf/1503.01817v2)                                                                                |      | a total of 100 million media objects, of which approximately 99.2 million are photos and 0.8 million are videos |
|         VSDial-CN          |  120K (Image), 1.2M(Audio)  | 120K  |  1.2M  |                                                        [VILAS: EXPLORING THE EFFECTS OF VISION AND LANGUAGE CONTEXT IN AUTOMATIC SPEECH RECOGNITION](https://arxiv.org/pdf/2305.19972v2)                                                         |      |                                                                                                                 |
|         AISHELL-2          |              -              |   -   |   1M   |                                                                    [AISHELL-2: Transforming Mandarin ASR Research Into Industrial Scale](https://arxiv.org/pdf/1808.10583v2)                                                                     |      |                                                                                                                 |
|         AISHELL-1          |              -              |   -   |  128K  |                                                             [AISHELL-1: AN OPEN-SOURCE MANDARIN SPEECH CORPUS AND A SPEECH RECOGNITION BASELINE](https://arxiv.org/pdf/1709.05522v1)                                                             |      |                                                                                                                 |
|          WavCaps           |         403K(Audio)         | 403K  |  403K  |                                                [WavCaps: A ChatGPT-Assisted Weakly-Labelled Audio Captioning Dataset for Audio-Language Multimodal Research](https://arxiv.org/pdf/2303.17395v2)                                                 |      |                                                                                                                 |


## Datasets of Multimodal Instruction Tuning


## Datasets of In-Context Learning


## Datasets of Multimodal Chain-of-Thought


## Datasets of Multimodal RLHF


## Benchmarks for Evaluation
